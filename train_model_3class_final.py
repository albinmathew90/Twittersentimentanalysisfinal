{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bced804-4798-45b8-b730-63804aca58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸš€ SENTIMENT140 (BALANCED) â€” 3-CLASS MODEL TRAINING\n",
      "=================================================================\n",
      "ğŸ“‚ Loading dataset...\n",
      "âœ… Loaded dataset: 2,400,000 tweets\n",
      "ğŸ“Š Shape: (2400000, 6)\n",
      "\n",
      "ğŸ“ˆ Raw label distribution:\n",
      "target\n",
      "0    800000\n",
      "2    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ§¹ Preprocessing data...\n",
      "ğŸ”„ Mapping sentiment labels (0, 2, 4 â†’ 0, 1, 2)...\n",
      "ğŸ—‘ï¸ Removed 0 invalid or missing rows\n",
      "ğŸ§¼ Cleaning tweets (may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400000/2400000 [00:22<00:00, 105036.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸ Removed 2,881 empty tweets after cleaning\n",
      "ğŸ—‘ï¸ Removed 799,926 duplicates (kept neutrals)\n",
      "\n",
      "âœ… Preprocessing complete!\n",
      "ğŸ“Š Final samples: 1,597,193\n",
      "ğŸ“Š Label distribution:\n",
      "target\n",
      "2    798642\n",
      "0    798477\n",
      "1        74\n",
      "Name: count, dtype: int64\n",
      "ğŸ“Š Current label distribution:\n",
      "target\n",
      "2    798642\n",
      "0    798477\n",
      "1        74\n",
      "Name: count, dtype: int64\n",
      "âš™ï¸ Oversampling neutral tweets to balance dataset...\n",
      "âœ… New label distribution:\n",
      "target\n",
      "2    798642\n",
      "1    798608\n",
      "0    798477\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ”¢ Creating TF-IDF features...\n",
      "ğŸ“Š Training samples: 1,916,581\n",
      "ğŸ“Š Test samples: 479,146\n",
      "ğŸ”„ Fitting TF-IDF vectorizer...\n",
      "âœ… TF-IDF Feature shape: (1916581, 20000)\n",
      "\n",
      "ğŸ¤– Training Logistic Regression (multiclass)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model training complete!\n",
      "\n",
      "ğŸ“Š Evaluating model...\n",
      "\n",
      "ğŸ¯ Test Accuracy: 85.32%\n",
      "\n",
      "ğŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7957    0.7556    0.7751    159695\n",
      "     Neutral     0.9966    1.0000    0.9983    159722\n",
      "    Positive     0.7678    0.8038    0.7854    159729\n",
      "\n",
      "    accuracy                         0.8532    479146\n",
      "   macro avg     0.8534    0.8531    0.8529    479146\n",
      "weighted avg     0.8534    0.8532    0.8529    479146\n",
      "\n",
      "ğŸ”¢ Confusion Matrix:\n",
      "[[120667    190  38838]\n",
      " [     0 159722      0]\n",
      " [ 30985    349 128395]]\n",
      "\n",
      "ğŸ’¾ Saving model to: sentiment_model_3class.pkl\n",
      "âœ… Model saved successfully!\n",
      "ğŸ“¦ Model size: 1.44 MB\n",
      "\n",
      "=================================================================\n",
      "âœ… PIPELINE COMPLETE â€” FINAL ACCURACY: 85.32%\n",
      "ğŸ’¾ Model saved as: sentiment_model_3class.pkl\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=========================================================\n",
    "SENTIMENT140 - 3 CLASS SENTIMENT MODEL TRAINING (FINAL)\n",
    "=========================================================\n",
    "\n",
    "Classes:\n",
    "- 0 â†’ Negative ğŸ˜¡\n",
    "- 2 â†’ Neutral ğŸ˜\n",
    "- 4 â†’ Positive ğŸ˜Š\n",
    "\n",
    "Dataset:\n",
    "- training_balanced.csv  (merged Sentiment140 + neutral_tweets.csv)\n",
    "\n",
    "Output:\n",
    "- sentiment_model_3class.pkl  (includes model + vectorizer)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Enable tqdm progress bars\n",
    "tqdm.pandas()\n",
    "\n",
    "# ==========================================================\n",
    "# SENTIMENT MODEL TRAINER CLASS\n",
    "# ==========================================================\n",
    "class SentimentModelTrainer:\n",
    "    def __init__(self, dataset_path, model_output_path=\"sentiment_model_3class.pkl\"):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.model_output_path = model_output_path\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        self.df = None\n",
    "\n",
    "    # -------------------- LOAD DATA --------------------\n",
    "    def load_data(self):\n",
    "        print(\"ğŸ“‚ Loading dataset...\")\n",
    "\n",
    "        columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "        try:\n",
    "            self.df = pd.read_csv(\n",
    "                self.dataset_path,\n",
    "                encoding=\"latin-1\",\n",
    "                names=columns,\n",
    "                header=None\n",
    "            )\n",
    "            print(f\"âœ… Loaded dataset: {len(self.df):,} tweets\")\n",
    "            print(f\"ğŸ“Š Shape: {self.df.shape}\")\n",
    "            print(f\"\\nğŸ“ˆ Raw label distribution:\\n{self.df['target'].value_counts()}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"âŒ File not found: {self.dataset_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading dataset: {str(e)}\")\n",
    "\n",
    "    # -------------------- CLEAN TEXT --------------------\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean tweet text (light cleaning to preserve neutral content).\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)       # Remove URLs\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)                 # Remove mentions\n",
    "        text = re.sub(r\"#\", \"\", text)                    # Keep hashtag words\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)       # Remove non-ASCII\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()         # Normalize spaces\n",
    "        return text\n",
    "\n",
    "    # -------------------- PREPROCESS --------------------\n",
    "    def preprocess_data(self):\n",
    "        print(\"\\nğŸ§¹ Preprocessing data...\")\n",
    "        self.df = self.df[[\"target\", \"text\"]]\n",
    "\n",
    "        print(\"ğŸ”„ Mapping sentiment labels (0, 2, 4 â†’ 0, 1, 2)...\")\n",
    "\n",
    "        # Convert labels to numeric (fixes mixed type issue)\n",
    "        self.df[\"target\"] = pd.to_numeric(self.df[\"target\"], errors=\"coerce\")\n",
    "\n",
    "        # Map Sentiment140 labels to our internal 3-class scheme\n",
    "        self.df[\"target\"] = self.df[\"target\"].map({0: 0, 2: 1, 4: 2})\n",
    "\n",
    "        before = len(self.df)\n",
    "        self.df.dropna(subset=[\"target\", \"text\"], inplace=True)\n",
    "        after = len(self.df)\n",
    "        print(f\"ğŸ—‘ï¸ Removed {before - after:,} invalid or missing rows\")\n",
    "\n",
    "        print(\"ğŸ§¼ Cleaning tweets (may take a few minutes)...\")\n",
    "        self.df[\"cleaned_text\"] = self.df[\"text\"].astype(str).progress_apply(self.clean_text)\n",
    "\n",
    "        before = len(self.df)\n",
    "        self.df = self.df[self.df[\"cleaned_text\"].str.len() > 0]\n",
    "        print(f\"ğŸ—‘ï¸ Removed {before - len(self.df):,} empty tweets after cleaning\")\n",
    "\n",
    "        # âš ï¸ Keep neutrals even if similar\n",
    "        before = len(self.df)\n",
    "        self.df = self.df[~((self.df[\"target\"] == 1) & (self.df.duplicated(subset=[\"cleaned_text\"])))]\n",
    "        print(f\"ğŸ—‘ï¸ Removed {before - len(self.df):,} duplicates (kept neutrals)\")\n",
    "\n",
    "        print(\"\\nâœ… Preprocessing complete!\")\n",
    "        print(f\"ğŸ“Š Final samples: {len(self.df):,}\")\n",
    "        print(f\"ğŸ“Š Label distribution:\\n{self.df['target'].value_counts()}\")\n",
    "\n",
    "       # Balance dataset safely (only if imbalance > 1.5x)\n",
    "        counts = self.df[\"target\"].value_counts()\n",
    "        print(f\"ğŸ“Š Current label distribution:\\n{counts}\")\n",
    "\n",
    "        max_count = counts.max()\n",
    "        min_count = counts.min()\n",
    "\n",
    "        if min_count / max_count < 0.5:\n",
    "    # Not enough neutrals â€” oversample them instead\n",
    "            print(\"âš™ï¸ Oversampling neutral tweets to balance dataset...\")\n",
    "            neutral_df = self.df[self.df[\"target\"] == 1]\n",
    "            repeats = int(max_count / len(neutral_df))\n",
    "            neutral_df = pd.concat([neutral_df] * repeats, ignore_index=True)\n",
    "            self.df = pd.concat([\n",
    "                self.df[self.df[\"target\"] != 1],\n",
    "                neutral_df\n",
    "            ], ignore_index=True)\n",
    "            self.df = self.df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            print(\"âš–ï¸ Dataset already fairly balanced.\")\n",
    "\n",
    "        print(f\"âœ… New label distribution:\\n{self.df['target'].value_counts()}\")\n",
    "        \n",
    "\n",
    "    # -------------------- FEATURE CREATION --------------------\n",
    "    def create_features(self):\n",
    "        print(\"\\nğŸ”¢ Creating TF-IDF features...\")\n",
    "\n",
    "        X = self.df[\"cleaned_text\"]\n",
    "        y = self.df[\"target\"]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        print(f\"ğŸ“Š Training samples: {len(X_train):,}\")\n",
    "        print(f\"ğŸ“Š Test samples: {len(X_test):,}\")\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=20000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=5,\n",
    "            max_df=0.85,\n",
    "            stop_words=\"english\"\n",
    "        )\n",
    "\n",
    "        print(\"ğŸ”„ Fitting TF-IDF vectorizer...\")\n",
    "        X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = self.vectorizer.transform(X_test)\n",
    "\n",
    "        print(f\"âœ… TF-IDF Feature shape: {X_train_tfidf.shape}\")\n",
    "        return X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "\n",
    "    # -------------------- TRAIN MODEL --------------------\n",
    "    def train_model(self, X_train, y_train):\n",
    "        print(\"\\nğŸ¤– Training Logistic Regression (multiclass)...\")\n",
    "        self.model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            solver=\"lbfgs\",\n",
    "            multi_class=\"multinomial\",\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        self.model.fit(X_train, y_train)\n",
    "        print(\"âœ… Model training complete!\")\n",
    "\n",
    "    # -------------------- EVALUATE --------------------\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        print(\"\\nğŸ“Š Evaluating model...\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"\\nğŸ¯ Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "        print(\"\\nğŸ“‹ Classification Report:\")\n",
    "        print(\n",
    "            classification_report(\n",
    "                y_test,\n",
    "                y_pred,\n",
    "                labels=[0, 1, 2],\n",
    "                target_names=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"ğŸ”¢ Confusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "        print(cm)\n",
    "        return acc\n",
    "\n",
    "    # -------------------- SAVE MODEL --------------------\n",
    "    def save_model(self):\n",
    "        print(f\"\\nğŸ’¾ Saving model to: {self.model_output_path}\")\n",
    "        model_data = {\n",
    "            \"model\": self.model,\n",
    "            \"vectorizer\": self.vectorizer,\n",
    "            \"feature_names\": self.vectorizer.get_feature_names_out()\n",
    "        }\n",
    "        joblib.dump(model_data, self.model_output_path)\n",
    "        print(f\"âœ… Model saved successfully!\")\n",
    "\n",
    "        import os\n",
    "        size = os.path.getsize(self.model_output_path) / (1024 * 1024)\n",
    "        print(f\"ğŸ“¦ Model size: {size:.2f} MB\")\n",
    "\n",
    "    # -------------------- RUN PIPELINE --------------------\n",
    "    def run_pipeline(self):\n",
    "        print(\"=\"*65)\n",
    "        print(\"ğŸš€ SENTIMENT140 (BALANCED) â€” 3-CLASS MODEL TRAINING\")\n",
    "        print(\"=\"*65)\n",
    "\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        X_train, X_test, y_train, y_test = self.create_features()\n",
    "        self.train_model(X_train, y_train)\n",
    "        acc = self.evaluate_model(X_test, y_test)\n",
    "        self.save_model()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*65)\n",
    "        print(f\"âœ… PIPELINE COMPLETE â€” FINAL ACCURACY: {acc*100:.2f}%\")\n",
    "        print(f\"ğŸ’¾ Model saved as: {self.model_output_path}\")\n",
    "        print(\"=\"*65)\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN FUNCTION\n",
    "# ==========================================================\n",
    "def main():\n",
    "    DATASET_PATH = \"training_balanced.csv\"   # your merged dataset\n",
    "    MODEL_OUTPUT_PATH = \"sentiment_model_3class.pkl\"\n",
    "\n",
    "    trainer = SentimentModelTrainer(DATASET_PATH, MODEL_OUTPUT_PATH)\n",
    "    trainer.run_pipeline()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74008a3e-8d04-47e7-a651-ae03da2ae1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa11e86-a40d-424b-b086-38e8a62c85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… Unique targets in dataset:\", self.df[\"target\"].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
